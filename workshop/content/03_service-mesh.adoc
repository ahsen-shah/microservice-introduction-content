:GUID: %guid%
:APPS: %cluster_subdomain%
:USER: %user%
:PASSWORD: %password%
:openshift_console_url: %openshift_console_url%
:user: %user%
:password: %password%

//:REPO_PREFIX: https://raw.githubusercontent.com/sa-mw-dach/opentour-2022-servicemesh/main
:REPO_PREFIX: https://raw.githubusercontent.com/sa-mw-dach/microservice-introduction-

:markup-in-source: verbatim,attributes,quotes
:source-highlighter: rouge

== Service Mesh

In this workshop we'll have a look at how a Service Mesh can help us to enhance the Security, Observability and Resiliency of our microservices. And all that without the need to add any libraries or to write infrastructure-related code.

## The sample apps

We have 3 sample apps, called Service A, B and C.

* Service A: Python app which calls Service B +
https://github.com/sa-mw-dach/microservice-introduction-app-a
* Service B: TypeScript/Deno app which calls Service C +
https://github.com/sa-mw-dach/microservice-introduction-app-b
* Service C: Java app with endpoints to simulate failures +
https://github.com/sa-mw-dach/microservice-introduction-app-c

## OpenShift Service Mesh preparation

In the cluster Service Mesh is already installed. You can check it in the dedicated Service Mesh namespace:

//TODO
[%autowidth]
|===
h|Select Project|`{USER}-service-mesh-system`
|===

Check the operators installed in the Admin perspective, Operators. All Service Mesh related Operators should be there and ready.

## Using the Service Mesh

### Deploy the sample apps

First, switch to the apps project:
[source,subs="attributes"]
```
oc project {USER}-service-mesh-app
```

Then apply the deployment files for the apps:

[source,subs="attributes"]
```
oc create -f {REPO_PREFIX}-servicemesh/main/kubernetes/a-deploy.yml
oc create -f {REPO_PREFIX}-servicemesh/main/kubernetes/b-deploy.yml
oc create -f {REPO_PREFIX}-servicemesh/main/kubernetes/c-v1-deploy.yml
oc create -f {REPO_PREFIX}-servicemesh/main/kubernetes/c-v2-deploy.yml
```

Check the pods - all pods should be running and you should see in the READY column "2/2". Why 2? In the pod are 2 containers - one for the app and one for the Envoy Sidecar.

### Create a Gateway for Ingress

Now we create a Gateway and expose our service-a.

[source,subs="attributes"]
```
oc create -f {REPO_PREFIX}-servicemesh/main/kubernetes/gateway.yml
oc get route istio-ingressgateway -n istio-system
ROUTE=...
curl $ROUTE/service-a
```

If the services respond correctly, continue.

## Canary Releases

Traffic shaping allows us to release new software versions as "Canary releases" to **avoid the risk of a Big Bang / All at Once approach**. This is the first use case we'll have a look at.

We already have two versions of service-c deployed. At the moment the traffic goes 50%/50%, the default "round robin" behavior of service routing in Kubernetes.

With a Service Mesh, we can finetune this behavior. First we inform the Service Mesh about our two versions, using a _DestinationRule_: +
[source,subs="attributes"]
oc create -f {REPO_PREFIX}-servicemesh/main/destination-rules.yml

Then we can start to shift the traffic. Open 2 terminals. 

**Terminal 1:**
[source,subs="attributes"]
```
ROUTE=...
while true; do curl $ROUTE/service-a; sleep 0.5; done
```

**Terminal 2:**

1. 100% traffic goes to our "old" version 1 +
[source,subs="attributes"]
oc create -f {REPO_PREFIX}-servicemesh/main/canary/1-vs-v1.yml
2. We start the canary release by sending 10% of traffic to version 2 +
[source,subs="attributes"]
oc replace -f {REPO_PREFIX}-servicemesh/main/canary/2-vs-v1_and_v2_90_10.yml
3. We are happy with version 2 and increase the traffic to 50% +
[source,subs="attributes"]
oc replace -f {REPO_PREFIX}-servicemesh/main/canary/3-vs-v1_and_v2_50_50.yml
4. Finally we send 100% of the traffic to version 2 +
[source,subs="attributes"]
oc replace -f {REPO_PREFIX}-servicemesh/main/canary/4-vs-v2.yml

While applying steps 1-4, check Kiali and Jaeger. Here you have great Observability without any libraries or coding*. You can open Jaeger and Kiali from the OpenShift Console (Networing Routes).

_(*) The Envoy Sidecar automatically injects tracing headers and sends traffic metadata to Kiali and Jaeger. For the Distributed Tracing, you must propagate the tracing headers when doing calls to other services. See https://istio.io/latest/docs/tasks/observability/distributed-tracing/overview/[Istio Header Propagation]._

### Circuit Breaker and Retry

**Terminal 1:**
[source,subs="attributes"]
```
ROUTE=...
while true; do curl $ROUTE/service-a; sleep 0.5; done
```

In Terminal 2, let's reset the VirtualService from our former Canary release and scale the service-c-v1 down to zero replicas and service-c-v2 up to 2 replicas.

**Terminal 2:**
[source,subs="attributes"]
```
oc replace -f {REPO_PREFIX}-servicemesh/main/circuit-breaker/1-vs.yml
oc scale deploy/service-c-v1 --replicas 0
oc scale deploy/service-c-v2 --replicas 2
```

Now connect to service-c and let it crash... in a separate terminal, run

**Terminal 3:**
[source,subs="attributes"]
```
oc get pod
POD_NAME=....
oc port-forward pod/$POD_NAME 8080:8080
```

Let the port-forwarding of Terminal 3 open, go back to Terminal 2 and let one app of service-c crash: +
`curl localhost:8080/crash`

See what happens in Terminal 1 with the curl loop.

Now apply the Circuit Breaker (check what happens), then the Retry policy.

**Terminal 2:** +
[source,subs="attributes"]
oc replace -f {REPO_PREFIX}-servicemesh/main/circuit-breaker/2-destination-rules.yml

Better, but still some errors. Let's apply the retry policy.

**Terminal 2:** +
[source,subs="attributes"]
oc replace -f {REPO_PREFIX}-servicemesh/main/circuit-breaker/3-vs-retry.yml

Finally repair the crashed service.

**Terminal 2:** +
`curl localhost:8080/repair`

After ~10 seconds the repaired pod gets traffic (Circuit Breaker goes from open to close).

**Congratulations, you made it!!**